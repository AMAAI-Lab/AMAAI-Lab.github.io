<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Publications</title>
  <meta name="description" content="AMAAI Lab -- Publications.">
  <link rel="stylesheet" href="/css/main.css">
  <link rel="canonical" href="/publications/">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
<link rel="shortcut icon" type ="image/x-icon" href="/images/favicon.ico">



</head>


  <body>

    <div class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container-fluid">
	<div class="navbar-header">
	  <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar-collapse-1" aria-expanded="false">
		<span class="sr-only">Toggle navigation</span>
		<span class="icon-bar"></span>
		<span class="icon-bar"></span>
		<span class="icon-bar"></span>
	  </button>
	
    <a class="navbar-brand" href="/">AMAAI Lab @ SUTD</a>
	</div>
	<div class="collapse navbar-collapse" id="navbar-collapse-1">
	  <ul class="nav navbar-nav navbar-right">
		<li><a href="/">Home</a></li>
		<li><a href="/team">Team</a></li>
		<li><a href="/vacancies">Vacancies</a></li>
		<li><a href="/publications">Publications</a></li>
		<li><a href="/research">Research</a></li>
	  </ul>
	</div>
  </div>
</div>

<!-- <li><a href="/pictures">(Pics)</a></li> -->


    <div class="container-fluid">
      <div class="row">
        <article class="post">

  <header class="post-header">
    <h1 class="post-title">Publications</h1>
  </header>

  <div class="post-content">
    
<h2 class="bibliography">2005</h2>
<ol class="bibliography"><li><span id="49">Herremans, D. (2005). <i>Tabu Search voor de optimalisatie van muzikale fragmenten: Vol. MSc Business Engineer Management Information Systems</i> [MSc]. Unversity of Antwerp.</span></li></ol>
<h2 class="bibliography">2010</h2>
<ol class="bibliography"><li><span id="herremans2010drupal">Herremans, D. (2010). <i>Drupal 6: Ultimate Community Site Guide</i>. Sun Flare Ltd.</span></li></ol>
<h2 class="bibliography">2011</h2>
<ol class="bibliography"><li><span id="101">Herremans, D., &amp; Sörensen, K. (2011). <i>A Variable Neighborhood Search Algorithm for Composing First Species Counterpoint Musical Fragments</i> [Working paper, University of Antwerp, Faculty of Applied Economics]. <i>2011017</i>(Technical Report 2011:017- UA, Faculty of Applied Economics). http://ideas.repec.org/p/ant/wpaper/2011017.html</span></li></ol>
<h2 class="bibliography">2012</h2>
<ol class="bibliography"><li><span id="246">Herremans, D., &amp; Sörensen, K. (2012). <i>Composing counterpoint musical scores with variable neighborhood search</i>.</span></li>
<li><span id="RePEc:ant:wpaper:2012020">Herremans, D., &amp; Sörensen, K. (2012). <i>Composing Fifth Species Counterpoint Music With Variable Neighborhood Search</i> [Technical Report 2012:020- UA, Faculty of Applied Economics]. <i>2012020</i>. http://ideas.repec.org/p/ant/wpaper/2012020.html</span></li>
<li><span id="herremans2012">Herremans, D., &amp; Sörensen, K. (2012). Composing first species counterpoint musical scores with a variable neighbourhood search algorithm. <i>Journal of Mathematics and the Arts</i>, <i>6</i>, 169–189. https://doi.org/10.1080/17513472.2012.738554</span></li></ol>
<h2 class="bibliography">2013</h2>
<ol class="bibliography"><li><span id="Herremans2013">Herremans, D., &amp; Sörensen, K. (2013). Composing Fifth Species Counterpoint Music With A Variable Neighborhood Search Algorithm. <i>Expert Systems with Applications</i>, <i>40</i>. https://doi.org/10.1016/j.eswa.2013.05.071</span></li>
<li><span id="173">Herremans, D., Martens, D., &amp; Sörensen, K. (2013). <i>Dance Hit Song Science</i> (6th ed.).</span></li>
<li><span id="247">Herremans, D., Sörensen, K., &amp; Conklin, D. (2013). <i>First species counterpoint generation with VNS and vertical viewpoints</i>.</span></li>
<li><span id="antor13">Herremans, D., &amp; Sörensen, K. (2013). FuX, an Android app that generates counterpoint. <i>IEEE Symposium on Computational Intelligence for Creativity and Affective Computing (CICAC)</i>, 48–55. https://doi.org/10.1109/CICAC.2013.6595220</span></li></ol>
<h2 class="bibliography">2014</h2>
<ol class="bibliography"><li><span id="5">Herremans, D. (2014). <i>Compose=Compute - Computer Generation And Classification Of Music Through Operations Research Methods</i> (p. 250). Universitas.</span></li>
<li><span id="herremans2014dance">Herremans, D., Martens, D., &amp; Sörensen, K. (2014). Dance hit song prediction [Special Issue on Music and Machine Learning]. <i>Journal of New Music Research</i>, <i>43</i>, 302. http://ideas.repec.org/p/ant/wpaper/2014003.html</span></li>
<li><span id="245">Herremans, D., Sörensen, K., &amp; Conklin, D. (2014). <i>First species counterpoint generation with VNS and vertical viewpoints</i>.</span></li>
<li><span id="243">Herremans, D., Weisser, S., Sörensen, K., &amp; Conklin, D. (2014). <i>Generating structured music using quality metrics based on Markov models</i> [Technical Report 2014:019- UA, Faculty of Applied Economics]. <i>2014019</i>. https://ideas.repec.org/p/ant/wpaper/2014019.html</span></li>
<li><span id="RePEc:ant:wpaper:2014001">Herremans, D., Martens, D., &amp; Sörensen, K. (2014). <i>Looking into the minds of Bach, Haydn and Beethoven: Classification and generation of composer-specific music</i> [Technical Report 2014:001- UA, Faculty of Applied Economics]. <i>2014001</i>. http://ideas.repec.org/p/ant/wpaper/2014001.html</span></li>
<li><span id="255">Herremans, D., Weisser, S., Sörensen, K., &amp; Conklin, D. (2014). Markov Based Quality Metrics For Generating Structured Music With Optimization Techniques. <i>Digital Music Research Network (DMNR+9)</i>.</span></li>
<li><span id="244">Herremans, D., Sörensen, K., &amp; Conklin, D. (2014). Sampling the extrema from statistical models of music with variable neighbourhood search. <i>ICMC/SMC</i>.</span></li></ol>
<h2 class="bibliography">2015</h2>
<ol class="bibliography"><li><span id="29">Herremans, D., Sörensen, K., &amp; Martens, D. (2015). Classification and generation of composer-specific music using global feature models and variable neighborhood search. <i>Computer Music Journal</i>, <i>39</i>, 91.</span></li>
<li><span id="79">Herremans, D. (2015). Compose = compute. <i>4OR</i>, <i>13</i>, 335–336. https://doi.org/10.1007/s10288-015-0282-y</span></li>
<li><span id="42">Herremans, D., Martens, D., Sörensen, K., &amp; Meredith, D. (2015). Composer Classification Models for Music-Theory Building. In <i>Computational Music Analysis</i>. Springer. https://doi.org/0.1007/978-3-319-25931-4</span></li>
<li><span id="82">Agres, K., Bigo, L., Herremans, D., &amp; Conklin, D. (2015). The effect of repetitive structure on enjoyment and altered states in uplifting trance music. <i>2nd International Conference on Music and Consciousness (MUSCON 2), Brighton</i>.</span></li>
<li><span id="35">Balliauw, M., Herremans, D., Palhazi Cuervo, D., &amp; Sörensen, K. (2015). Generating Fingerings for Polyphonic Piano Music with a Tabu Search Algorithm. In T. Collins, D. Meredith, &amp; A. Volk (Eds.), <i>Mathematics and Computation in Music</i> (Vol. 9110, pp. 149–160). Springer International Publishing. https://doi.org/10.1007/978-3-319-20603-5_15</span></li>
<li><span id="30">Herremans, D., Weisser, S., Sörensen, K., &amp; Conklin, D. (2015). <i>Generating music with an optimization algorithm using a Markov based objective function</i>.</span></li>
<li><span id="31">Herremans, D., Weisser, S., Sörensen, K., &amp; Conklin, D. (2015). Generating structured music for bagana using quality metrics based on Markov models. <i>Expert Systems With Applications</i>, <i>42 (21)</i>, 424–7435.</span></li></ol>
<h2 class="bibliography">2016</h2>
<ol class="bibliography"><li><span id="67">Agres, K., Bigo, L., Herremans, D., &amp; Conklin, D. (2016). <i>The Effect of Repetitive Structure on Enjoyment in Uplifting Trance Music</i> (pp. 280–282).</span></li>
<li><span id="65">Herremans, D., &amp; Chew, E. (2016). MorpheuS: Automatic music generation with recurrent pattern constraints and tension profiles [Technical report]. <i>IEEE TENCON</i>, <i>ISSN 2043-0167</i>.</span></li>
<li><span id="57">Herremans, D., &amp; Chew, E. (2016). <i>MorpheuS: constraining structure in automatic music generation</i>. Leibniz Centre for Informatik. http://drops.dagstuhl.de/opus/volltexte/2016/6141/pdf/dagrep_v006_i002_p147_s16092.pdf#page=22</span></li>
<li><span id="48">Herremans, D., &amp; Chew, E. (2016). <i>Music generation with structural constraints: an operations research approach</i> (pp. 37–39).</span></li>
<li><span id="53">Herremans, D., &amp; Chew, E. (2016). <i>Tension ribbons: Quantifying and visualising tonal tension</i> (Vol. 2, pp. 8–18).</span></li>
<li><span id="66">Cunha, N., A., S., &amp; Herremans, D. (2016). <i>Uma abordagem baseada em programação linear inteira para a geração de solos de guitarra</i>.</span></li></ol>
<h2 class="bibliography">2017</h2>
<ol class="bibliography"><li><span id="93">Herremans, D., Chuan, C.-H., &amp; Chew, E. (2017). A Functional Taxonomy of Music Generation Systems. <i>ACM Computing Surveys</i>, <i>50</i>, 30. https://doi.org/10.1145/3108242</span></li>
<li><span id="97">Cunha, N., A., S., &amp; Herremans, D. (2017). Generating guitar solos by integer programming. <i>Journal of the Operational Research Society</i>, 971–985. https://doi.org/https://doi.org/10.1080/01605682.2017.1390528</span></li>
<li><span id="77">Agres, K., Herremans, D., Bigo, L., &amp; Conklin, D. (2017). Harmonic Structure Predicts the Enjoyment of  Uplifting Trance Music. <i>Frontiers in Psychology, Cognitive Science</i>, <i>7</i>. http://journal.frontiersin.org/article/10.3389/fpsyg.2016.01999/full</span></li>
<li><span id="98">Herremans, D., &amp; Bergmans, T. (2017). <i>Hit Song Prediction Based on Early Adopter Data and Audio Features</i>.</span></li>
<li><span id="86">Herremans, D., Yang, S., Chuan, C.-H., Barthet, M., &amp; Chew, E. (2017). <i>IMMA-Emo: A Multimodal Interface for Visualising Score- and Audio-synchronised Emotion Annotations</i>.</span></li>
<li><span id="84">Herremans, D., &amp; Chuan, C.-H. (2017). <i>Modeling Musical Context with Word2vec</i> (Vol. 1, pp. 11–18). https://doi.org/10.13140/RG.2.2.22227.99364/1</span></li>
<li><span id="94">Herremans, D., &amp; Chew, E. (2017). MorpheuS: generating structured music with constrained patterns and tension. <i>IEEE Transactions on Affective Computing</i>, <i>PP (In Press)</i>. https://doi.org/10.1109/TAFFC.2017.2737984</span></li>
<li><span id="78">Herremans, D., &amp; Chuan, C.-H. (2017). A multi-modal platform for semantic music analysis: visualizing audio- and score-based tension. <i>11th International Conference  on Semantic Computing IEEE ICSC 2017</i>.</span></li>
<li><span id="99">Agres, K., &amp; Herremans, D. (2017). <i>Music and Motion-Detection: A Game Prototype for Rehabilitation and Strengthening in the Elderly</i>.</span></li>
<li><span id="43">Balliauw, M., Herremans, D., Palhazi Cuervo, D., &amp; Sörensen, K. (2017). A variable neighborhood search algorithm to generate piano fingerings for polyphonic sheet music. <i>International Transactions in Operational Research, Special Issue on Variable Neighbourhood Search</i>, <i>24</i>, 509–535. https://doi.org/10.1111/itor.12211</span></li>
<li><span id="100">Herremans, D., &amp; Lauwers, W. (2017). <i>Visualizing the evolution of alternative hit charts</i>.</span></li></ol>
<h2 class="bibliography">2018</h2>
<ol class="bibliography"><li><span id="129">Cheuk, K. W., BT, B., Roig, G., &amp; Herremans, D. (2018). <i>Blacklisted speaker identification using triplet neural networks</i>. http://mce.csail.mit.edu/pdfs/SUTD_description.pdf</span></li>
<li><span id="132">Chuan, C.-H., Agres, K., &amp; Herremans, D. (2018). From Context to Concept: Exploring Semantic Relationships in Music with Word2Vec. <i>Neural Computing and Applications</i>. https://doi.org/10.1007/s00521-018-3923-1</span></li>
<li><span id="119">Agus, N., Anderson, H., Chen, J. M., Lui, S., &amp; Herremans, D. (2018). Minimally Simple Binaural Room Modelling Using a Single Feedback Delay Network. <i>Journal of the Audio Engineering Society</i>, <i>66</i>, 791–807. https://doi.org/https://doi.org/10.17743/jaes.2018.0045</span></li>
<li><span id="104">Chuan, C.-H., &amp; Herremans, D. (2018). Modeling temporal tonal relations in polyphonic music through deep networks with a novel image-based representation. <i>The Thirty-Second AAAI Conference on Artificial Intelligence</i>.</span></li>
<li><span id="128">Sokolovskis, J., Herremans, D., &amp; Chew, E. (2018). A Novel Interface for the Graphical Analysis of Music Practice Behaviours. <i>Frontiers in Psychology - Human-Media Interaction</i>, <i>9</i>. https://doi.org/10.3389/fpsyg.2018.02292</span></li>
<li><span id="116">Herremans, D., &amp; Chew, E. (2018). <i>O.R. and music generation</i>. <i>45</i>. https://www.informs.org/ORMS-Today/Public-Articles/February-Volume-45-Number-1/O.R.-and-music-generation</span></li>
<li><span id="117">Agus, N., Anderson, H., Chen, J. M., Lui, S., &amp; Herremans, D. (2018). Perceptual evaluation of measures of spectral variance. <i>Journal of the Acoustical Society of America</i>, <i>143</i>, 3300–3311. https://doi.org/10.1121/1.5040484</span></li>
<li><span id="2018">Agus, N. (2018). <i>Real-Time Binaural Auralization: Vol. PhD</i> [PhD]. Singapore University of Technology and Design.</span></li>
<li><span id="134">Lin, K. W. E., BT, B., Koh, E., Lui, S., &amp; Herremans, D. (2018). Singing Voice Separation Using a Deep Convolutional Neural Network Trained by Ideal Binary Mask and Cross Entropy. <i>Neural Computing and Applications</i>. https://doi.org/10.1007/s00521-018-3933-z</span></li>
<li><span id="120">Agres, K., &amp; Herremans, D. (2018). The Structure of Chord Progressions Influences Listeners’ Enjoyment and Absorptive States in EDM. <i>15th International Conference on Music Perception and Cognition</i>.</span></li></ol>
<h2 class="bibliography">2019</h2>
<ol class="bibliography"><li><span id="9998">Hee, H. I., BT, B., Karunakaran, A., Herremans, D., Teoh, O. H., Lee, K. P., Teng, S. S., Lui, S., &amp; Chen, J. M. (2019). Development of Machine Learning for asthmatic and healthy voluntary  cough - a proof of concept study. <i>Applied Sciences</i>, <i>9</i>. https://doi.org/10.3390/app9142833</span></li>
<li><span id="2097">Lee-Leon, A., Yuen, C., &amp; Herremans, D. (2019). Doppler Invariant Demodulation for Shallow Water Acoustic Communications Using Deep Belief Networks. <i>16th IEEE Asia Pacific Wireless Communications Symposium (APWCS)</i>.</span></li>
<li><span id="2098">Herremans, D., &amp; Chuan, C.-H. (2019). The emergence of deep learning: new opportunities for music and audio technologies [Editorial]. <i>Neural Computing and Applications</i>. https://doi.org/https://doi.org/10.1007/s00521-019-04166-0</span></li>
<li><span id="2099">Lee-Leon, A., Yuen, C., &amp; Herremans, D. (2019). A Hybrid Fuzzy Logic-Neural Network Approach For Multi-path Separation Of Underwater Acoustic Signals. <i>89th IEEE Vehicular Technology Conference</i>. https://doi.org/10.1109/VTCSpring.2019.8746614</span></li>
<li><span id="2100">Agres, K., Bigo, L., &amp; Herremans, D. (2019). The impact of musical structure on enjoyment and absorptive listening states in trance music. In <i>Music and Consciousness 2 - Worlds, Practices, Modalities</i> (Editors: Ruth Herbert, David Clarke, and Eric Clarke). Oxford University Press. https://global.oup.com/academic/product/music-and-consciousness-2-9780198804352?cc=us&amp;lang=en&amp;#</span></li>
<li><span id="2101">Cheuk, K. W., BT, B., Roig, G., &amp; Herremans, D. (2019). Latent space representation for multi-target speaker detection and identification with a sparse dataset using Triplet neural networks. <i>IEEE Automatic Speech Recognition and Understanding Workshop (ASRU 2019)</i>.</span></li>
<li><span id="2102">Luo, Y. J., Agres, K., &amp; Herremans, D. (2019). Learning Disentangled Representations of Timbre and Pitch for Musical Instrument Sounds Using Gaussian Mixture Variational Autoencoders. <i>ISMIR</i>. https://arxiv.org/abs/1906.08152</span></li>
<li><span id="122">Sturm, B., Ben-Tal, O., Monaghan, U., Collins, N., Herremans, D., Chew, E., Hadjeres, G., Deruty, E., &amp; Pachet, F. (2019). Machine Learning Research that Matters for Music  Creation: A Case Study. <i>Journal of New Music Research</i>, <i>48</i>, 36–55. https://doi.org/10.1080/09298215.2018.1515233</span></li>
<li><span id="2103">Guo, R., Herremans, D., &amp; Magnusson, T. (2019). Midi Miner – A Python library for tonal tension and track classification. <i>ISMIR - Late Breaking Demo</i>.</span></li>
<li><span id="2104">Phuong, T. H. T., Herremans, D., &amp; Roig, G. (2019). Multimodal Deep Models for Predicting Affective Responses Evoked by Movies. <i>The 2nd International Workshop on Computer Vision for Physiological Measurement as Part of ICCV. Seoul, South Korea. 2019</i>.</span></li>
<li><span id="2105">Cheuk, K. W., Agres, K., &amp; Herremans, D. (2019). nnAudio: A PyTorch Audio Processing Tool Using 1D Convolution neural networks. <i>ISMIR - Late Breaking Demo</i>.</span></li>
<li><span id="2106">Agres, K., Lui, S., &amp; Herremans, D. (2019). A novel music-based game with motion capture to support cognitive and motor function in the elderly. <i>IEEE Conference on Games</i>.</span></li>
<li><span id="2107">Herremans, D., &amp; Chew, E. (2019). Towards emotion based music generation: A tonal tension model based on the spiral array. <i>Proceedings of Cognitive Science (CogSci)</i>.</span></li>
<li><span id="2108">BT, B., Lin, K. W. E., Lui, S., Chen, J. M., &amp; Herremans, D. (2019). Towards robust audio spoofing detection: a detailed comparison of traditional and learned features. <i>IEEE Access</i>, <i>7</i>, 84229–84241. https://doi.org/10.1109/ACCESS.2019.2923806</span></li></ol>
<h2 class="bibliography">2020</h2>
<ol class="bibliography"><li><span id="2085">BT, B., Aslim, E. J., Ng, Y. S. L., Kuo, T. L. C., Chen, J. S., Herremans, D., Ng, L. G., &amp; Chen, J. M. (2020). Acoustic prediction of flowrate: varying liquid jet stream onto a free surface. <i>IEEE International Conference on Signal Processing and Communications (SPCOM)</i>.</span></li>
<li><span id="2086">BT, B., Hee, H. I., Teoh, O. H., Lee, K. P., Kapoor, S., Herremans, D., &amp; Chen, J. M. (2020). Asthmatic versus healthy child classification based on cough and vocalised /a:/ sounds [Jasa Express Letters]. <i>The Journal of the Acoustical Society of America (JASA)</i>, <i>148, EL253</i>. https://doi.org/10.1121/10.0001933</span></li>
<li><span id="2087">Pham, Q.-H. (2020). <i>Data-driven 3D Scene Understanding: Vol. PhD</i> [Master's thesis]. SUTD.</span></li>
<li><span id="2088">Nahar, F., Agres, K., BT, B., &amp; Herremans, D. (2020). A dataset and classification model for Malay, Hindi, Tamil and Chinese music. <i>13th Workshop on Music and Machine Learning (MML) as Part of ECML/PKDD</i>.</span></li>
<li><span id="2089">Tan, H. H., Luo, Y. J., &amp; Herremans, D. (2020). <i>Generative Modelling for Controllable Audio Synthesis of Expressive Piano Performance</i>. arXiv:2006.09833. https://arxiv.org/abs/2006.09833</span></li>
<li><span id="2090">Cheuk, K. W., Agres, K., &amp; Herremans, D. (2020). The impact of Audio input representations on neural network based music transcription. <i>Proceedings of the International Joint Conference on Neural Networks (IJCNN)</i>. https://arxiv.org/abs/2001.09989</span></li>
<li><span id="2091">Tan, H. H., &amp; Herremans, D. (2020). Music FaderNets: Controllable Music Generation Based On High-Level Features via Low-Level Feature Modelling. <i>ISMIR</i>.</span></li>
<li><span id="2019">Cheuk, K. W., Anderson, H., Agres, K., &amp; Herremans, D. (2020). nnAudio: An on-the-fly GPU Audio to Spectrogram Conversion Toolbox Using 1D Convolution Neural Networks. <i>IEEE Access</i>. https://doi.org/10.1109/ACCESS.2020.3019084</span></li>
<li><span id="2092">Garg, K., Singh, A., Herremans, D., &amp; Lall, B. (2020). PerceptionGAN: Real-world image construction from provided text through perceptual understanding. <i>4th Int. Conf. on Imaging, Vision and Pattern Recognition (IVPR), and 9th Int. Conf. on Informatics, Electronics &amp; Vision (ICIEV)</i>.</span></li>
<li><span id="2093">Cheuk, K. W., Luo, Y. J., BT, B., Roig, G., &amp; Herremans, D. (2020). Regression-based music emotion prediction using triplet neural networks. <i>Proceedings of the International Joint Conference on Neural Networks (IJCNN)</i>. https://arxiv.org/abs/2001.09988</span></li>
<li><span id="2094">Luo, Y. J., Hsu, C.-C., Agres, K., &amp; Herremans, D. (2020). Singing voice conversion with disentangled representations of singer and vocal technique using variational autoencoders. <i>IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP)</i>. https://arxiv.org/abs/1912.02613</span></li>
<li><span id="2095">Luo, Y. J., Cheuk, K. W., Nakano, T., Goto, M., &amp; Herremans, D. (2020). Unsupervised disentanglement of pitch and timbre for isolated musical instrument sounds. <i>Proceedings of the International Society of Music Information Retrieval (ISMIR)</i>. https://arxiv.org/abs/1906.08152</span></li>
<li><span id="2096">Guo, R., Simpson, I., Magnusson, T., Kiefer, C., &amp; Herremans, D. (2020). A variational autoencoder for music generation controlled by tonal tension. <i>Joint Conference on AI Music Creativity (CSMC + MuMe)</i>. https://arxiv.org/abs/2010.06230</span></li></ol>
<h2 class="bibliography">2021</h2>
<ol class="bibliography"><li><span id="2021">Herremans, D. (2021). aiSTROM - A roadmap for developing a successful AI strategy. <i>IEEE Access</i>. https://doi.org/10.1109/ACCESS.2021.3127548</span></li>
<li><span id="2074">Phuong, T. H. T., BT, B., Roig, G., &amp; Herremans, D. (2021). AttendAffectNet – Emotion Prediction of Movie Viewers Using Multimodal Fusion with Self-attention. <i>Sensors. Special Issue on Intelligent Sensors: Sensor Based Multi-Modal Emotion Recognition</i>. https://doi.org/10.3390/s21248356</span></li>
<li><span id="2075">Phuong, T. H. T., BT, B., Herremans, D., &amp; Roig, G. (2021). AttendAffectNet: Self-Attention based Networks for Predicting Affective Responses from Movies. <i>Proceedings of the International Conference on Pattern Recognition (ICPR2020)</i>.</span></li>
<li><span id="2076">T, B. B., Hee, H. I., Kapoor, S., Teoh, O. H., Teng, S. S., Lee, K. P., Herremans, D., &amp; Chen, J. M. (2021). Deep Neural Network Based Respiratory Pathology Classification Using Cough Sounds. <i>Sensors</i>, <i>21</i>, 5555. https://doi.org/10.3390/s21165555</span></li>
<li><span id="2020">Cheuk, K. W., Luo, Y. J., Benetos, E., &amp; Herremans, D. (2021). The Effect of Spectrogram Reconstructions on Automatic Music Transcription:An Alternative Approach to Improve Transcription Accuracy. <i>Proceedings of the International Conference on Pattern Recognition (ICPR2020)</i>.</span></li>
<li><span id="2077">Wang, K., Tekler, Z., Cheah, L., Herremans, D., &amp; Blessing, L. (2021). Evaluating the Effectiveness of an Augmented Reality Game Promoting Environmental Action. <i>Sustainability</i>, <i>13</i>, 13912. https://doi.org/10.3390/su132413912</span></li>
<li><span id="2078">Makris, D., Agres, K., &amp; Herremans, D. (2021). Generating Lead Sheets with Affect: A Novel Conditional seq2seq Framework. <i>Proceedings of the International Joint Conference on Neural Networks (IJCNN)</i>. https://arxiv.org/abs/2104.13056</span></li>
<li><span id="2079">Guo, Z., Makris, D., &amp; Herremans, D. (2021). Hierarchical Recurrent Neural Networks for Conditional Melody Generation with Long-term Structure. <i>Proceedings of the International Joint Conference on Neural Networks (IJCNN)</i>. https://arxiv.org/abs/2102.09794</span></li>
<li><span id="2080">Agres, K., Schaefer, R., Volk, A., Van Hooren, S., Holzapfel, A., Dalla Bella, S., Müller, M., de Witte, M., Herremans, D., Ramirez Melendez, R., Neerincx, M., Ruiz, S., Meredith, D., Dimitriadis, T., &amp; Magee, W. (2021). Music, Computing, and Health: A roadmap for the current and future roles of music technology for healthcare and well-being. <i>Music &amp; Science</i>. https://doi.org/https://doi.org/10.1177/205920432199770</span></li>
<li><span id="2081">Kroonenberg, P., &amp; Herremans, D. (2021). Musical stylometry: Characterisation of music. In <i>Multivariate Humanities</i>. Springer. https://doi.org/10.1007/978-3-030-69150-9</span></li>
<li><span id="2082">Cheuk, K. W., Su, L., &amp; Herremans, D. (2021). ReconVAT: A Semi-Supervised Automatic Music Transcription Framework for Low-Resource Real-World Data. <i>ACM Multimedia</i>. https://dl.acm.org/doi/10.1145/3474085.3475405</span></li>
<li><span id="2083">Cheuk, K. W., Luo, Y. J., Benetos, E., &amp; Herremans, D. (2021). Revisiting the Onsets and Frames Model with Additive Attention. <i>Proceedings of the International Joint Conference on Neural Networks (IJCNN)</i>.</span></li>
<li><span id="2084">Lee-Leon, A., Yuen, C., &amp; Herremans, D. (2021). Underwater Acoustic Communication Receiver Using Deep Belief Network. <i>IEEE Transactions on Communications</i>, 1–1. https://doi.org/10.1109/TCOMM.2021.3063353</span></li></ol>
<h2 class="bibliography">2022</h2>
<ol class="bibliography"><li><span id="2061">Clarke, C. J., Chowdhury, J., BT, B., Priyadarshinee, P., Lim, C. M. Y., Tan, I. F. X., Herremans, D., &amp; Chen, J. M. (2022). Computationally Efficient Physics Approximating Neural Networks for Highly Nonlinear Maps. <i>2022 International Conference on Research in Adaptive and Convergent Systems</i>. https://doi.org/10.1145/3538641.3561501</span></li>
<li><span id="2062">Makris, D., Guo, Z., Kaliakatsos-Papakostas, N., &amp; Herremans, D. (2022). Conditional Drums Generation using Compound Word Representations. <i>EvoMUSART (EVO*) - Lecture Notes in Computer Science</i>.</span></li>
<li><span id="2063">Levers, O. D., Herremans, D., Dipankar, A., &amp; Blessing, L. (2022). Downscaling using Deep Convolutional Autoencoders, a case study for South East Asia. <i>Egusphere Preprint</i>. https://doi.org/https://doi.org/10.5194/egusphere-2022-234</span></li>
<li><span id="2064">Pham, Q.-H., Herremans, D., &amp; Roig, G. (2022). EmoMV: Affective Music-Video Correspondence Learning Datasets for Classification and Retrieval. <i>Information Fusion</i>. https://doi.org/https://doi.org/10.1016/j.inffus.2022.10.002</span></li>
<li><span id="2065">BT, B., Hee, H. I., Ming, C., Lin, Y., Priyadarshinee, P., Clarke, C. J., Herremans, D., &amp; Chen, J. M. (2022). A Gaussian mixture classifier model to differentiate respiratory symptoms using phonated /ɑ:/ sounds. <i>The 18th Australasian International Conference on Speech Science and Technology (SST)</i>. https://sst2022.com/a-gaussian-mixture-classifier-model-to-differentiate-respiratory-symptoms-using-phonated-a%cb%90-sounds/</span></li>
<li><span id="2066">Turian, J., Shier, J., Khan, H. R., Raj, B., Schuller, B. W., Steinmetz, C. J., Malloy, C., Tzanetakis, G., Velarde, G., McNally, K., Henry, M., Pinto, N., Noufi, C., Clough, C., Herremans, D., Fonseca, E., Engel, J., Salamon, J., Esling, P., … Bisk, Y. (2022). HEAR 2021: Holistic Evaluation of Audio Representations. <i>Proceedings of Machine Learning Research (PMLR): NeurIPS 2021 Competition Track</i>. https://arxiv.org/abs/2203.03022</span></li>
<li><span id="2067">Cheuk, K. W., Choi, K., Kong, Q., Li, B., Won, M., Hung, A., Wang, J.-C., &amp; Herremans, D. (2022). <i>Jointist: Joint Learning for Multi-instrument Transcription and Its Applications</i>. https://doi.org/https://doi.org/10.48550/arXiv.2206.10805</span></li>
<li><span id="2068">Kaliakatsos-Papakostas, N., Bastas, G., Makris, D., Herremans, D., Katsouros, V., &amp; Maragos, P. (2022). A Machine Learning Approach for MIDI to Guitar Tablature Conversion. <i>Sound and Music Computing Conference (SMC)</i>.</span></li>
<li><span id="2069">Guo, R., Simpton, I., Kiefer, C., Magnusson, T., &amp; Herremans, D. (2022). MusIAC: An extensible generative framework for Music Infilling Application with multi-level Control. <i>EvoMUSART</i>. https://arxiv.org/abs/2202.05528</span></li>
<li><span id="2070">Chua, P., Makris, D., Agres, K., Roig, G., &amp; Herremans, D. (2022). Predicting emotion from music videos: exploring the relative contribution of visual and auditory information to affective responses. <i>Arxiv Preprint</i>. https://arxiv.org/abs/2202.10453</span></li>
<li><span id="2071">Huang, J., Chia, Y. K., Yu, S., Yee, K., Küster, D., Krumhuber, E. G., Herremans, D., &amp; Roig, G. (2022). Single Image Video Prediction with Auto-Regressive GANs. <i>Sensors</i>, <i>22</i>, 3533. https://doi.org/10.3390/s22093533</span></li>
<li><span id="2072">Kwan, Y. H., Cheuk, K. W., &amp; Herremans, D. (2022). Understanding Audio Features via Trainable Basis Functions. <i>Arxiv Preprint</i>. https://arxiv.org/abs/2204.11437</span></li>
<li><span id="2073">Sockalingam, N., Lo, K., n, K. O., Herremans, D., Raghunath, N., Cancion, H. G. C., Kejun, H., Leong, H., Tan, J., Nizharzharudin, K., &amp; Pey, K. L. (2022). A white paper on cyberphysical learning. <i>White Paper, Singapore University of Technology and Design</i>. https://www.sutd.edu.sg/SUTD/media/SUTD/LSL_WhitePaper_Cyber-physical-Campus-Higher-Education.pdf</span></li></ol>
<h2 class="bibliography">2023</h2>
<ol class="bibliography"><li><span id="2013">Ong, J., &amp; Herremans, D. (2023). Constructing Time-Series Momentum Portfolios with Deep Multi-Task Learning. <i>Expert Systems with Applications</i>, <i>230</i>. https://doi.org/10.1016/j.eswa.2023.120587</span></li>
<li><span id="2056">Cheuk, K. W., Sawata, R., Uesaka, T., Murata, N., Takahashi, N., Takahashi, S., Herremans, D., &amp; Mitsufuji, Y. (2023). DiffRoll: Diffusion-based Generative Music Transcription with Unsupervised Pretraining Capability. <i>ICASSP</i>.</span></li>
<li><span id="2057">Guo, Z., Kang, J., &amp; Herremans, D. (2023). A Domain-Knowledge-Inspired Music Embedding Space and a Novel Attention Mechanism for Symbolic Music Modeling. <i>Proceedings of the 37th AAAI Conference on Artificial Intelligence</i>.</span></li>
<li><span id="2058">Melechovsky, J., Mehrish, A., Herremans, D., &amp; Sisman, B. (2023). Learning accent representation with multi-level VAE towards controllable speech synthesis. <i> IEEE Spoken Language Technology (SLT) Workshop</i>.</span></li>
<li><span id="2059">Koh, E., Cheuk, K. W., Heung, K. Y., Agres, K., &amp; Herremans, D. (2023). MERP: A Music Dataset with Emotion Ratings and Raters’ Profile Information. <i>Sensors - Intelligent Sensors</i>, <i>23</i>. https://doi.org/10.3390/s23010382</span></li>
<li><span id="2060">Zou, Y., &amp; Herremans, D. (2023). A Multimodal Model with Twitter Finbert Embeddings for Extreme Price Movement Prediction of Bitcoin. <i>Expert Systems with Applications</i>. https://doi.org/https://doi.org/10.1016/j.eswa.2023.120838</span></li></ol>
<h2 class="bibliography">2024</h2>
<ol class="bibliography"><li><span id="2043">Melechovsky, J., Mehrish, A., Sisman, B., &amp; Herremans, D. (2024). Accent Conversion in Text-To-Speech Using Multi-Level VAE and Adversarial Training. <i>Proc. of IEEE Tencon, Singapore</i>. https://arxiv.org/abs/2406.01018</span></li>
<li><span id="2023">Melechovsky, J., Mehrish, A., Sisman, B., &amp; Herremans, D. (2024). Accented Text-to-Speech Synthesis with a Conditional Variational Autoencoder. <i>Proc. of IEEE Tencon, Singapore</i>. https://arxiv.org/abs/2211.03316</span></li>
<li><span id="2044">Luo, J., Yang, X., &amp; Herremans, D. (2024). BandControlNet: Parallel Transformers-based Steerable Popular Music Generation with Fine-Grained Spatiotemporal Features. <i>ArXiv:2407.10462</i>. https://arxiv.org/abs/2407.10462</span></li>
<li><span id="2045">Lanzendörfer, L. A., Lu, T., Perraudin, N., Herremans, D., &amp; Wattenhofer, R. (2024). Coarse-to-Fine Text-to-Music Latent Diffusion. <i>Audio Imagination: NeurIPS 2024 Workshop</i>.</span></li>
<li><span id="2046">Melechovsky, J., Mehrish, A., Sisman, B., &amp; Herremans, D. (2024). DART: Disentanglement of Accent and Speaker Representation in Multispeaker Text-to-Speech. <i>Audio Imagination: NeurIPS 2024 Workshop</i>. https://arxiv.org/abs/2410.13342</span></li>
<li><span id="2047">Ong, J., &amp; Herremans, D. (2024). DeepUnifiedMom: Unified Time-series Momentum Portfolio Construction via Multi-Task Learning with Multi-Gate Mixture of Experts. <i>ArXiv:2406.08742</i>. https://arxiv.org/abs/2406.08742</span></li>
<li><span id="2048">Wang, K., &amp; Herremans, D. (2024). DisfluencySpeech – Single-Speaker Conversational Speech Dataset with Paralanguage. <i>Proc. of IEEE Tencon, Singapore</i>. https://arxiv.org/abs/2406.08820</span></li>
<li><span id="2049">Chow, D., &amp; Herremans, D. (2024). <i>Gamification and skills tree</i>. https://sutd.edu.sg/SUTD/media/SUTD/SUTD-campusX_CPL-Foresight-Report.pdf</span></li>
<li><span id="2050">Melechovsky, J., Roy, A., &amp; Herremans, D. (2024). MidiCaps — A large-scale MIDI dataset with text captions. <i>ISMIR</i>. https://arxiv.org/abs/2406.02255</span></li>
<li><span id="2051">Chopra, A., Roy, A., &amp; Herremans, D. (2024). MIRFLEX: Music Information Retrieval Feature Library for Extraction. <i>ISMIR, Late Breaking Demos</i>. https://arxiv.org/abs/2411.00469</span></li>
<li><span id="2052">Ong, J. (2024). <i>Modern Portfolio Construction with Advanced Deep Learning Models: Vol. PhD</i> [Master's thesis].</span></li>
<li><span id="2053">Melechovsky, J., Guo, Z., Ghosal, D., Majumder, N., Herremans, D., &amp; Poria, S. (2024). Mustango: Toward Controllable Text-to-Music Generation. <i>Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers). Pages 8293–8316</i>. https://doi.org/https://aclanthology.org/2024.naacl-long.pdf</span></li>
<li><span id="2054">Lam, P., Zhang, H., Chen, N. F., Sisman, B., &amp; Herremans, D. (2024). SNIPER Training: Variable Sparsity Rate Training For Text-To-Speech. <i>Proc. of IEEE Tencon, Singapore</i>. https://arxiv.org/abs/2211.07283</span></li>
<li><span id="2055">Kang, J., Poria, S., &amp; Herremans, D. (2024). Video2Music: Suitable Music Generation from Videos using an Affective Multimodal Transformer model. <i>Expert Systems with Applications</i>. https://doi.org/https://doi.org/10.1016/j.eswa.2024.123640</span></li></ol>
<h2 class="bibliography">2025</h2>
<ol class="bibliography"><li><span id="2024">Kang, J., &amp; Herremans, D. (2025). Are we there yet? A brief survey of Music Emotion Prediction Datasets, Models and Outstanding Challenges. <i>IEEE Transactions on Affective Computing</i>. https://arxiv.org/abs/2406.08809</span></li>
<li><span id="2025">Lanzendörfer, L. A., Lu, T., Perraudin, N., Herremans, D., &amp; Wattenhofer, R. (2025). Coarse-to-Fine Text-to-Music Latent Diffusion. <i>Proceedings of ICASSP</i>. https://openreview.net/pdf/b3dcd6d5d6c26679621a2e6c176455d59658c0a8.pdf</span></li>
<li><span id="2026">Tripathi, A., Patle, V., Jain, A., Pundir, A., Menon, S., Singh, A. K., &amp; Herremans, D. (2025). End-to-End Text-to-SQL with Dataset Selection: Leveraging LLMs for Adaptive Query Generation. <i>Proceedings of IJCNN, Rome, Italy</i>.</span></li>
<li><span id="2027">Guo, R., &amp; Herremans, D. (2025). An exploration of controllability in symbolic music infilling. <i>IEEE Access</i>. https://doi.org/0.1109/ACCESS.2025.3554648</span></li>
<li><span id="2022">Herremans, D., &amp; Low, K. W. (2025). Forecasting Bitcoin Volatility Spikes from Whale Transactions and Cryptoquant Data Using Synthesizer Transformer Models. <i>IEEE Access</i>, <i>13</i>, 117788–117807. https://doi.org/10.1109/ACCESS.2025.3584243</span></li>
<li><span id="2028">Bhandari, K., Chang, S., Lu, T., Enus, F. R., Bradshaw, L. B., Herremans, D., &amp; Colton, S. (2025). ImprovNet: Generating Controllable Musical Improvisations with Iterative Corruption Refinement. <i>Proceedings of IJCNN, Rome, Italy</i>. https://arxiv.org/abs/2502.04522</span></li>
<li><span id="2029">Roy, A., Liu, R., Lu, T., &amp; Herremans, D. (2025). JamendoMaxCaps: A Large Scale Music-caption Dataset with Imputed Metadata. <i>Proceedings of IJCNN, Rome, Italy</i>. https://arxiv.org/abs/2502.07461</span></li>
<li><span id="2030">Liu, R., Roy, A., &amp; Herremans, D. (2025). <i>Leveraging LLM Embeddings for Cross Dataset Label Alignment and Zero Shot Music Emotion Prediction</i>. https://arxiv.org/abs/2410.11522</span></li>
<li><span id="2031">Song, M., Pala, T. D., Jin, W., Zadeh, A., Li, C., Herremans, D., &amp; Poria, S. (2025). LLMs Can’t Handle Peer Pressure: Crumbling under Multi-Agent Social Interactions. <i>ArXiv:2508.18321</i>. https://arxiv.org/abs/2508.18321</span></li>
<li><span id="2032">Lu, T., Geist, C.-M., Melechovsky, J., Roy, A., &amp; Herremans, D. (2025). MelodySim: Measuring Melody-aware Music Similarity for Plagiarism Detection. <i>ArXiv:2505.20979</i>. https://arxiv.org/abs/2505.20979</span></li>
<li><span id="2033">Le, D.-V.-T., Bigo, L., Keller, M., &amp; Herremans, D. (2025). Natural Language Processing Methods for Symbolic Music Generation and Information Retrieval: a Survey. <i>ACM Computing Surveys</i>. https://doi.org/https://arxiv.org/abs/2402.17467</span></li>
<li><span id="2034">Lam, P., Zhang, H., Chen, N. F., Sisman, B., &amp; Herremans, D. (2025). PRESENT: Zero-Shot Text-to-Prosody Control. <i>IEEE Signal Processing Letters</i>. https://doi.org/10.1109/LSP.2025.3528359</span></li>
<li><span id="2035">Wei, M., Modrzejewski, M., Sivaraman, A., &amp; Herremans, D. (2025). <i>Prevailing Research Areas for Music AI in the Era of Foundation Models</i>. https://arxiv.org/abs/2409.09378</span></li>
<li><span id="2036">Herremans, D. (2025). <i> Royalties in the age of AI: paying artists for AI-generated songs</i>. https://www.wipo.int/web/wipo-magazine/articles/royalties-in-the-age-of-ai-paying-artists-for-ai-generated-songs-73739</span></li>
<li><span id="2037">Melechovsky, J., Mehrish, A., &amp; Herremans, D. (2025). SonicMaster: Towards Controllable All-in-One Music Restoration and Mastering. <i>ArXiv:2508.03448</i>. https://arxiv.org/abs/2508.03448</span></li>
<li><span id="2038">Chopra, A., Roy, A., &amp; Herremans, D. (2025). SonicVerse: Multi-Task Learning for Music Feature-Informed Captioning. <i>Proceedings of the 6th Conference on AI Music Creativity (AIMC 2025), Brussels, Belgium, September 10th - 12th, 2025</i>. https://arxiv.org/abs/2506.15154</span></li>
<li><span id="2039">Bhandari, K., Roy, A., Wang, K., Puri, G., Colton, S., &amp; Herremans, D. (2025). Text2midi: Generating Symbolic Music from Captions. <i>Proceedings of AAAI, Philadelphia</i>. https://www.arxiv.org/abs/2412.16526</span></li>
<li><span id="2040">Roy, A., Puri, G., &amp; Herremans, D. (2025). Text2midi-InferAlign: Improving Symbolic Music Generation with Inference-Time Alignment. <i>ArXiv:2505.12669</i>. https://arxiv.org/abs/2505.12669</span></li>
<li><span id="2041">Sockalingam, N., Lo, K., Teo, J., Wei, C. C., Chow, D., Herremans, D., Jun, M. L. M., Kurniawan, O., Wang, Y., &amp; Leong, P. K. (2025). Towards the future of education: cyber-physical learning. <i>Discover Education</i>, <i>4</i>, 1–16. https://doi.org/https://doi.org/10.1007/s44217-025-00474-x</span></li>
<li><span id="2042">Kang, J., &amp; Herremans, D. (2025). <i>Towards Unified Music Emotion Recognition across Dimensional and Categorical Models</i>. https://arxiv.org/abs/2502.03979</span></li></ol>


  </div>

</article>

      </div>
    </div>

    <div id="footer" class="panel">
  <div class="panel-footer">
	<div class="container-fluid">
	  <div class="row">
		<div class="col-sm-6">
			
		  <p>&copy 2025 AMAAI Lab. Site made with <a href="https://jekyllrb.com">Jekyll</a></p>
              <p>We are part of the <a href="https://www.sutd.edu.sg/istd">Information Systems Technology and Design (ISTD)</a> at <a href="https://www.sutd.edu.sg">Singapore University of Technology and Design</a>.</p>
            

		   <p>  </p><p>
            		  
            
		</div>
		<!--
		<div class="col-sm-4">
		  Funding:<br />
		  - <a href="http://www.nwo.nl/en/research-and-results/programmes/Talent+Scheme">Vidi </a> and <a href='https://www.fom.nl/en/news/press-releases/2016/11/18/28634/'>Projectruimte</a> grants from <a href="http://www.nwo.nl">NWO</a> <br />
		  - <a href="https://www.universiteitleiden.nl/en/research/research-projects/science/frontiers-of-nanoscience-nanofront">Frontier of Nanosciences</a>, a gravity program from <a href="http://www.nwo.nl">NWO</a>
          - <a href='https://www.universiteitleiden.nl/en/news/2017/08/two-erc-grants-for-leiden-physics'>ERC starting grant</a>
		</div>

		  
		<a href="/aboutwebsite.html">copy and  modify it for your own research group.</a>
		-->
		  
		<div class="col-sm-6">
		  Contact:<br />
		  DARES Lab, Level 3, Building 3, Singapore University of Technology and Design, 8 Somapah Rd, Singapore, 487372<br />
          (<a href="https://maps.app.goo.gl/1Y8VMr2WbGXLXGY2A">Maps</a>, <a href="https://www.sutd.edu.sg/contact-us/getting-around-sutd/">Directions</a>)
		</div>
	  </div>
	</div>
  </div>
</div>

<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
<script src="/js/bootstrap.min.js"></script>


  </body>

</html>
