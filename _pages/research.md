---
title: "AMAAI Lab - Research"
layout: textlay
excerpt: "AMAAI Lab -- Research"
sitemap: false
permalink: /research/
---

# Research

At the **Audio, Music, and AI (AMAAI) Lab** at SUTD, our mission is to advance artificial intelligence for music and audio.  
We design systems that can **understand, generate, and interact with music**, combining deep learning, signal processing, and music theory.  
Our research spans creative AI, music information retrieval, multimodal learning, and ethical AI in music.

---

### Text-to-Music Generation
<div style="display: flex; align-items: flex-start; gap: 20px;">
 <img src="{{ site.url }}{{ site.baseurl }}/images/mustango.jpg" alt="Mustango Framework" width="500"/>
  <p>
   We developed <b>Mustango</b>, a <i>music-domain-knowledge-inspired</i> text-to-music system based on diffusion models.  
   Unlike prior systems that rely only on general text prompts, Mustango enables <b>controllable music generation</b> with rich captions that specify <b>chords, beats, tempo, and key</b>.
   <br><br>
   ðŸ”— <a href="https://github.com/AMAAI-Lab/mustango" target="_blank">Code on GitHub</a>
   ðŸ”— <a href="https://arxiv.org/abs/2311.08355" target="_blank">Paper on arXiv</a>  
  </p>
</div>

---

### Video-to-Music Generation
<div style="display: flex; align-items: flex-start; gap: 20px;">
 <img src="{{ site.url }}{{ site.baseurl }}/images/video2music.png" alt="Video2music Framework" width="500"/>
  <p>
   Music often coexists with video and other modalities, yet most generative AI models cannot create music that <i>matches</i> a given video.  
   To address this, we developed <b>Video2Music</b>, a generative framework that produces music conditioned on <b>visual and semantic cues</b>.
  <br><br>
   ðŸ”— <a href="https://github.com/AMAAI-Lab/Video2Music" target="_blank">Code on GitHub</a>
   ðŸ”— <a href="https://arxiv.org/abs/2311.00968" target="_blank">Paper on arXiv</a>  
  </p>
</div>

---

### Music Emotion Recognition (MER)
We explore how music evokes emotions by developing multitask learning frameworks that combine **deep embeddings** with **music-theory-informed features** (e.g., chords and key signatures).  
This research enables richer emotion-aware music recommendation and generation.

---

### Automatic Music Transcription and Analysis
Through models such as **DiffRoll** and **nnAudio**, we push the limits of deep learning for transcription and symbolic analysis, including chord recognition, onset detection, and alignment between audio and score.

---

### Creative AI and Plagiarism Detection
With generative AI raising originality concerns, we design systems to detect similarity in **melody, harmony, timbre, and lyrics**.  
Our work supports both creative exploration and intellectual property protection.

---

### Foundational Music Models
We build large-scale **joint audioâ€“text embedding models** and **foundational music models** that power downstream tasks such as retrieval, captioning, recommendation, and adaptive generation.

---

### Open-Source Tools and Datasets
We actively contribute to the community with open-source projects such as **Mustango, Video2Music, Music FaderNets, DiffRoll, and MidiCaps**.  
These resources promote reproducibility and fuel innovation in music AI.

### ... and more.

<br><br>
