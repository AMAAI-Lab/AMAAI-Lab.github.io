---
title: "AMAAI Lab - Research"
layout: textlay
excerpt: "AMAAI Lab -- Research"
sitemap: false
permalink: /research/
---

# Research

At the **Audio, Music, and AI (AMAAI) Lab** at SUTD, our mission is to advance artificial intelligence for music and audio.  
We design systems that can **understand, generate, and interact with music**, combining deep learning, signal processing, and music theory.  
Our research spans creative AI, music information retrieval, multimodal learning, and ethical AI in music.

---

### Text-to-Music Generation
<div style="display: flex; align-items: flex-start; gap: 20px;">
 <img src="{{ site.url }}{{ site.baseurl }}/images/mustango.jpg" alt="Mustango Framework" width="300"/>
  <p>
    We develop controllable generative models, such as <b>Mustango</b>, that transform natural language prompts into music.  
    Our work enables fine-grained control over attributes like genre, mood, and instrumentation.
  </p>
</div>

---

### Video-to-Music and Multimodal Generation
Music often coexists with video and text. We build models like **Video2Music** and **MuVi**, which generate or analyze music conditioned on visual and semantic cues.

---

### Music Emotion Recognition (MER)
We explore how music evokes emotions by developing multitask learning frameworks that combine **deep embeddings** with **music-theory-informed features** (e.g., chords and key signatures).  
This research enables richer emotion-aware music recommendation and generation.

---

### Automatic Music Transcription and Analysis
Through models such as **DiffRoll** and **nnAudio**, we push the limits of deep learning for transcription and symbolic analysis, including chord recognition, onset detection, and alignment between audio and score.

---

### Creative AI and Plagiarism Detection
With generative AI raising originality concerns, we design systems to detect similarity in **melody, harmony, timbre, and lyrics**.  
Our work supports both creative exploration and intellectual property protection.

---

### Foundational Music Models
We build large-scale **joint audioâ€“text embedding models** and **foundational music models** that power downstream tasks such as retrieval, captioning, recommendation, and adaptive generation.

---

### Open-Source Tools and Datasets
We actively contribute to the community with open-source projects such as **Mustango, Video2Music, Music FaderNets, DiffRoll, and MidiCaps**.  
These resources promote reproducibility and fuel innovation in music AI.

### ... and more.
