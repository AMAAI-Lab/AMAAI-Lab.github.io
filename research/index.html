<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>AMAAI Lab - Research</title>
  <meta name="description" content="AMAAI Lab -- Research">
  <link rel="stylesheet" href="/css/main.css">
  <link rel="canonical" href="/research/">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
<link rel="shortcut icon" type ="image/x-icon" href="/images/favicon.ico">



</head>


  <body>

    <div class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container-fluid">
	<div class="navbar-header">
	  <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar-collapse-1" aria-expanded="false">
		<span class="sr-only">Toggle navigation</span>
		<span class="icon-bar"></span>
		<span class="icon-bar"></span>
		<span class="icon-bar"></span>
	  </button>
	
    <a class="navbar-brand" href="/">AMAAI Lab @ SUTD</a>
	</div>
	<div class="collapse navbar-collapse" id="navbar-collapse-1">
	  <ul class="nav navbar-nav navbar-right">
		<li><a href="/">Home</a></li>
		<li><a href="/team">Team</a></li>
		<li><a href="/vacancies">Vacancies</a></li>
		<li><a href="/publications">Publications</a></li>
		<li><a href="/research">Research</a></li>
	  </ul>
	</div>
  </div>
</div>

<!-- <li><a href="/pictures">(Pics)</a></li> -->


    <div class="container-fluid">
      <div class="row">
        <div id="textid" class="col-sm-12">
  <style>
.research-section {
  display: flex;
  align-items: flex-start;
  gap: 20px;
  flex-wrap: wrap;
}

.research-image {
  max-width: 500px;
  width: 100%;
  height: auto;
  border-radius: 8px;
  box-shadow: 0 4px 8px rgba(0,0,0,0.1);
}

.research-content {
  flex: 1;
  min-width: 300px;
}

.research-links {
  margin-top: 15px;
}

.research-links a {
  margin-right: 15px;
  margin-bottom: 10px;
  padding: 8px 16px;
  text-decoration: none;
  border-radius: 5px;
  transition: background-color 0.3s ease;
}

.research-links a:hover {
  background-color: #e2f0ff;
}

/* Mobile responsiveness */
@media (max-width: 768px) {
  .research-section {
    flex-direction: column;
    gap: 15px;
  }
  
  .research-image {
    max-width: 100%;
    align-self: center;
  }
  
  .research-content {
    min-width: unset;
  }
  
  .research-links a {
    margin-bottom: 10px;
    margin-right: 0;
  }
}

/* Tablet responsiveness */
@media (max-width: 1024px) and (min-width: 769px) {
  .research-image {
    max-width: 400px;
  }
}

/* Small mobile devices */
@media (max-width: 480px) {
  .research-section {
    gap: 10px;
  }
  
  .research-image {
    max-width: 100%;
  }
  
  .research-links a {
    font-size: 14px;
    padding: 6px 12px;
  }
}
</style>

<h1 id="research">Research</h1>

<p>At the <strong>Audio, Music, and AI (AMAAI) Lab</strong> at SUTD, our mission is to advance artificial intelligence for music and audio. We design systems that can <strong>understand, generate, and interact with music</strong>, combining deep learning, signal processing, and music theory. Our research spans creative AI, music information retrieval, multimodal learning, and ethical AI in music.</p>

<hr />

<h3 id="text-to-music-generation">Text-to-Music Generation</h3>

<div class="research-section">
  <p><img src="/images/mustango.jpg" alt="Mustango Framework" class="research-image" /></p>
  <div class="research-content">
    <p>We developed <strong>Mustango</strong>, a <em>music-domain-knowledge-inspired</em> text-to-music system based on diffusion models. Unlike prior systems that rely only on general text prompts, Mustango enables <strong>controllable music generation</strong> with rich captions that specify <strong>chords, beats, tempo, and key</strong>.</p>

    <div class="research-links">
      <p><a href="https://github.com/AMAAI-Lab/mustango" target="_blank">ðŸ”— Code on GitHub</a>
<a href="https://arxiv.org/abs/2311.08355" target="_blank">ðŸ”— Paper on arXiv</a></p>
    </div>
  </div>
</div>

<hr />

<h3 id="video-to-music-generation">Video-to-Music Generation</h3>
<div class="research-section">
  <p><img src="/images/video2music.png" alt="Video2music Framework" class="research-image" /></p>
  <div class="research-content">
    <p>Music often coexists with video and other modalities, yet most generative AI models cannot create music that <em>matches</em> a given video. To address this, we developed <strong>Video2Music</strong>, a generative framework that produces music conditioned on <strong>visual and semantic cues</strong>.</p>

    <div class="research-links">
      <p><a href="https://github.com/AMAAI-Lab/Video2Music" target="_blank">ðŸ”— Code on GitHub</a>
<a href="https://arxiv.org/abs/2311.00968" target="_blank">ðŸ”— Paper on arXiv</a></p>
    </div>
  </div>
</div>

<hr />

<h3 id="music-emotion-recognition-mer">Music Emotion Recognition (MER)</h3>
<div class="research-section">
  <p><img src="/images/music2emo.jpg" alt="Music2Emo Framework" class="research-image" /></p>
  <div class="research-content">
    <p>We study how music evokes emotions by unifying categorical (e.g., happy, sad) and dimensional (e.g., valenceâ€“arousal) models. Our framework, released as <strong>Music2Emo</strong>, integrates deep embeddings with music-theory-informed features (e.g., chords, key) and leverages multitask learning to improve generalization across datasets. This work enables richer emotion-aware music recommendation and generation.</p>

    <div class="research-links">
      <p><a href="https://github.com/AMAAI-Lab/Music2Emotion" target="_blank">ðŸ”— Code on GitHub</a>
<a href="https://arxiv.org/abs/2502.03979" target="_blank">ðŸ”— Paper on arXiv</a></p>
    </div>
  </div>
</div>

<hr />

<h3 id="automatic-music-transcription-and-analysis">Automatic Music Transcription and Analysis</h3>
<p>Through models such as <strong>DiffRoll</strong> and <strong>nnAudio</strong>, we push the limits of deep learning for transcription and symbolic analysis, including chord recognition, onset detection, and alignment between audio and score.</p>

<hr />

<h3 id="creative-ai-and-plagiarism-detection">Creative AI and Plagiarism Detection</h3>
<p>With generative AI raising originality concerns, we design systems to detect similarity in <strong>melody, harmony, timbre, and lyrics</strong>.<br />
Our work supports both creative exploration and intellectual property protection.</p>

<hr />

<h3 id="foundational-music-models">Foundational Music Models</h3>
<p>We build large-scale <strong>joint audioâ€“text embedding models</strong> and <strong>foundational music models</strong> that power downstream tasks such as retrieval, captioning, recommendation, and adaptive generation.</p>

<hr />

<h3 id="open-source-tools-and-datasets">Open-Source Tools and Datasets</h3>
<p>We actively contribute to the community with open-source projects such as <strong>Mustango, Video2Music, Music FaderNets, DiffRoll, and MidiCaps</strong>.<br />
These resources promote reproducibility and fuel innovation in music AI.</p>

<h3 id="-and-more">â€¦ and more.</h3>

<p><br /><br /></p>

</div>

      </div>
    </div>

    <div id="footer" class="panel">
  <div class="panel-footer">
	<div class="container-fluid">
	  <div class="row">
		<div class="col-sm-6">
			
		  <p>&copy 2025 AMAAI Lab. Site made with <a href="https://jekyllrb.com">Jekyll</a></p>
              <p>We are part of the <a href="https://www.sutd.edu.sg/istd">Information Systems Technology and Design (ISTD)</a> at <a href="https://www.sutd.edu.sg">Singapore University of Technology and Design</a>.</p>
            

		   <p>  </p><p>
            		  
            
		</div>
		<!--
		<div class="col-sm-4">
		  Funding:<br />
		  - <a href="http://www.nwo.nl/en/research-and-results/programmes/Talent+Scheme">Vidi </a> and <a href='https://www.fom.nl/en/news/press-releases/2016/11/18/28634/'>Projectruimte</a> grants from <a href="http://www.nwo.nl">NWO</a> <br />
		  - <a href="https://www.universiteitleiden.nl/en/research/research-projects/science/frontiers-of-nanoscience-nanofront">Frontier of Nanosciences</a>, a gravity program from <a href="http://www.nwo.nl">NWO</a>
          - <a href='https://www.universiteitleiden.nl/en/news/2017/08/two-erc-grants-for-leiden-physics'>ERC starting grant</a>
		</div>

		  
		<a href="/aboutwebsite.html">copy and  modify it for your own research group.</a>
		-->
		  
		<div class="col-sm-6">
		  Contact:<br />
		  DARES Lab, Level 3, Building 3, Singapore University of Technology and Design, 8 Somapah Rd, Singapore, 487372<br />
          (<a href="https://maps.app.goo.gl/1Y8VMr2WbGXLXGY2A">Maps</a>, <a href="https://www.sutd.edu.sg/contact-us/getting-around-sutd/">Directions</a>)
		</div>
	  </div>
	</div>
  </div>
</div>

<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
<script src="/js/bootstrap.min.js"></script>


  </body>

</html>
